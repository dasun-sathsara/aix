############# conversation_manager.py #############

from datetime import UTC, datetime
from pathlib import Path

from pydantic import BaseModel, Field

from api.api_interface import API
from data.message import Message


class Config(BaseModel):
    print_code_block: bool = Field(True, description='Include the code block in the output markdown file.')
    print_error_block: bool = Field(True, description='Include the error block in the output markdown file.')
    print_file_content: bool = Field(True, description='Include file content in the output markdown file.')
    save_conversation: bool = Field(False, description='Whether to save the conversation.')
    markdown_language: str = Field('plaintext', description='The markdown language for the output markdown file.')


class ConversationHandler:
    def __init__(self, api: API, config: Config):
        self._api = api
        self._config = config
        self._printing_history: list[Message] = []
        self._timestamp = datetime.now(tz=UTC).strftime('%Y-%m-%d_%H-%M-%S')

    def set_config(self, config: Config) -> None:
        """Update the configuration."""

        self._config = config

    def send_message(
        self, prompt: str, code_block: str | None = None, error_block: str | None = None, files: list[str] | None = None
    ) -> None:
        """Send a message to the API and save the conversation."""

        llm_prompt = self._create_llm_prompt(prompt, code_block, error_block, files)
        user_prompt = self._create_user_prompt(prompt, code_block, error_block, files)
        self._printing_history.append(user_prompt)

        response = self._api.send_message(llm_prompt)
        self._printing_history.append(Message(role='Assistant', content=response.message))

        self.save_to_disk('output.md')

        if self._config.save_conversation:
            self.save_to_disk(f'outputs/{self._timestamp}.md')

    def _create_user_prompt(
        self, prompt: str, code_block: str | None = None, error_block: str | None = None, files: list[str] | None = None
    ) -> Message:
        """Create the printing history for the given inputs."""

        message_parts = []

        if files and self._config.print_file_content:
            message_parts.extend(self._process_files(files))

        if code_block and self._config.print_code_block:
            message_parts.append(self._format_block('Code Block', code_block))

        if error_block and self._config.print_error_block:
            message_parts.append(self._format_block('Error Block', error_block))

        message_parts.append(self._format_block('Prompt', prompt.strip()))

        return Message(role='User', content='\n'.join(message_parts))

    def _create_llm_prompt(
        self, prompt: str, code_block: str | None = None, error_block: str | None = None, files: list[str] | None = None
    ) -> str:
        """Create the LLM prompt from the given inputs."""

        prompt_parts = []

        if files:
            prompt_parts.extend(self._process_files(files))
        if code_block:
            prompt_parts.append(self._format_block('Code Block', code_block))
        if error_block:
            prompt_parts.append(self._format_block('Error Block', error_block))

        prompt_parts.append(self._format_block('Prompt', prompt.strip()))

        return '\n\n'.join(filter(None, prompt_parts)).strip()

    def _process_files(self, files: list[str]) -> list[str]:
        """Process the list of files and return their contents."""

        file_contents = []
        for file in files:
            try:
                content = Path(file).read_text(encoding='utf-8').strip()
                if self._config.print_file_content:
                    file_contents.append(f'**File Name**: {file}\n**File Content:**\n```{self._config.markdown_language}\n{content}\n```\n')
            except OSError as e:
                print(f'Error reading file {file}: {e}')
        return file_contents

    def _format_block(self, block_type: str, content: str) -> str:
        """Format a block of content with markdown."""

        markdown_language = self._config.markdown_language

        if block_type == 'Prompt':
            markdown_language = 'plaintext'

        if block_type == 'Error Block':
            markdown_language = 'plaintext'

        return f'**{block_type}:**\n```{markdown_language}\n{content.strip()}\n```'

    def clear_session(self) -> None:
        """Clear the session."""

        self._api.clear_session()
        self._printing_history.clear()
        self._timestamp = datetime.now(tz=UTC).strftime('%Y-%m-%d_%H-%M-%S')

    def rewind(self, count: int = 1) -> None:
        """Rewind the conversation by a specified number of user-assistant pairs."""

        if count < 1:
            raise ValueError('Rewind count must be at least 1')

        self._api.rewind(count)

        for _ in range(min(count, len(self._printing_history) // 2)):
            self._printing_history.pop()  # Remove latest Assistant message
            self._printing_history.pop()  # Remove latest User message

    def save_to_disk(self, path: str) -> None:
        """Save the conversation to a file."""

        Path(path).parent.mkdir(parents=True, exist_ok=True)

        with Path(path).open('w') as f:
            for message in self._printing_history:
                if message.role == 'User':
                    f.write(f'\n## Me\n{message.content}\n')
                else:
                    f.write(f'\n## You\n{message.content}\n')
                    f.write('---')


###################################################

############# main.py #############

from api.gemini_api import GeminiAPI
from conversation_manager import Config, ConversationHandler
from utils.configs import gemini_settings
from utils.functions import read_system_prompt


def main():
    gemini = GeminiAPI(
        api_key=gemini_settings.api_key,
        model=gemini_settings.models.GEMINI_FLASH.value,
        system_prompt=read_system_prompt('general_assistant'),
    )

    app = ConversationHandler(
        api=gemini, config=Config(print_code_block=False, print_error_block=False, print_file_content=False, markdown_language='python')
    )

    code_block = r"""
    def add(a, b):
        return a + b
    """

    error_block = r"""
    Traceback (most recent call last):
      File "main.py", line 1, in <module>
        def add(a, b):
      File "main.py", line 2, in add
        return a + b
    TypeError: unsupported operand type(s) for +: 'int' and 'str'
    """

    app.send_message('Describe what is in my prompt', code_block, error_block)
    app.send_message('Is that so?')
    app.send_message('Describe our whole conversation from one prompt to the next')


if __name__ == '__main__':
    main()


###################################

############# api\api_interface.py #############

from abc import ABC, abstractmethod

from data.api_response import APIResponse
from data.message import Message


class API(ABC):
    @abstractmethod
    def send_message(self, message: str) -> 'APIResponse':
        pass

    @abstractmethod
    def clear_session(self) -> None:
        pass

    @abstractmethod
    def get_chat_history(self) -> list['Message']:
        pass

    @abstractmethod
    def rewind(self, count: int = 1) -> None:
        pass


################################################

############# api\gemini_api.py #############

from typing import Any

import google.generativeai as genai  # type: ignore

from data.api_response import APIResponse
from data.message import Message

from .api_interface import API


class GeminiAPI(API):
    """
    A specialized API wrapper for interacting with Google's Generative AI models.
    """

    def __init__(self, api_key: str, model: str, system_prompt: str, temperature: float = 0.85, max_tokens: int = 8192) -> None:
        """
        Initializes the GeminiAPI with the provided configuration.
        Configures the API key and sets up the model with specified parameters.
        """

        genai.configure(api_key=api_key)

        self._model_name: str = model
        self._system_prompt: str = system_prompt
        self._conversation: list[Message] = []

        self._config: dict[str, Any] = {
            'temperature': temperature,
            'max_output_tokens': max_tokens,
            'top_p': 0.95,
            'top_k': 40,
            'response_mime_type': 'text/plain',
        }

        self._model = self._create_model()
        self._chat_session = self._model.start_chat(history=[])

    def _create_model(self) -> genai.GenerativeModel:
        """
        Creates and returns a GenerativeModel instance with the configured parameters.
        """

        return genai.GenerativeModel(
            model_name=self._model_name,
            generation_config=self._config,
            system_instruction=self._system_prompt,
        )

    def send_message(self, message: str) -> APIResponse:
        """
        Sends a message to the AI model and returns the response along with token usage.
        """

        try:
            self._conversation.append(Message(role='User', content=message))
            response = self._chat_session.send_message(message)
            self._conversation.append(Message(role='Assistant', content=response.text))

            usage_metadata = response.to_dict().get('usage_metadata', {})
            return APIResponse(
                message=response.text,
                prompt_tokens=int(usage_metadata.get('prompt_token_count', 0)),
                output_tokens=int(usage_metadata.get('candidates_token_count', 0)),
            )
        except Exception as e:
            raise RuntimeError(f'Error sending message: {e!s}') from e

    def clear_session(self) -> None:
        """
        Resets the chat session and clears the conversation history.
        """

        self._chat_session = self._model.start_chat(history=[])
        self._conversation.clear()

    def get_chat_history(self) -> list[Message]:
        """
        Returns a copy of the current conversation history.
        """

        return self._conversation.copy()

    def rewind(self, count: int = 1) -> None:
        """
        Rewinds the conversation by a specified number of user-assistant pairs.
        """

        if count < 1:
            raise ValueError('Rewind count must be at least 1')

        for _ in range(min(count, len(self._conversation) // 2)):
            self._chat_session.rewind()
            self._conversation.pop()  # Remove latest Assistant message
            self._conversation.pop()  # Remove latest User message


#############################################

############# api\generic_api.py #############

from typing import Literal

import openai
import tiktoken

from data.api_response import APIResponse
from data.message import Message

from .api_interface import API


class GenericAPI(API):
    """
    A generic API wrapper for interacting with OpenAI-compatible models.
    """

    def __init__(
        self, api_key: str, model: str, system_prompt: str, base_url: str, temperature: float = 0.85, max_tokens: int = 8192
    ) -> None:
        """
        Initializes the GenericAPI class with the provided configuration,
        including setting the API key, base URL, and conversational parameters.
        """

        self._client = openai.OpenAI(api_key=api_key, base_url=base_url)
        self._model_name: str = model
        self._system_prompt: str = system_prompt
        self._temperature: float = temperature
        self._max_tokens: int = max_tokens
        self._conversation: list[dict] = []

        # Add the system prompt to the conversational history
        self._conversation.insert(0, {'role': 'system', 'content': self._system_prompt})

    def rewind(self, count: int = 1) -> None:
        """
        Rewinds the conversation by a specified number of user-assistant pairs.
        """

        if count < 1:
            raise ValueError('Rewind count must be at least 1')

        for _ in range(min(count, len(self._conversation) // 2)):
            self._conversation.pop()  # Remove latest Assistant message
            self._conversation.pop()  # Remove latest User message

    def get_chat_history(self) -> list[Message]:
        """
        Returns a copy of the current conversation history.
        """

        history = []
        for message in self._conversation:
            if message['role'] == 'system':
                continue

            role: Literal['User', 'Assistant'] = 'User' if message['role'] == 'user' else 'Assistant'
            history.append(Message(role=role, content=message['content']))

        return history

    def clear_session(self) -> None:
        """
        Resets the chat session and clears the conversation history.
        """

        self._conversation.clear()

    def _count_tokens(self, text: str) -> int:
        """
        Counts the number of tokens in a given text.
        """

        encoding = tiktoken.get_encoding('o200k_base')
        return len(encoding.encode(text))

    def send_message(self, message: str) -> APIResponse:
        """
        Sends a message to the AI model and returns the response along with token usage.
        """

        try:
            self._conversation.append({'role': 'user', 'content': message})

            response = self._client.chat.completions.create(  # type: ignore
                model=self._model_name,
                messages=self._conversation,
                temperature=self._temperature,
                max_tokens=self._max_tokens,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0.67,
                response_format={'type': 'text'},
            )

            response_text = response.choices[0].message.content
            self._conversation.append({'role': 'assistant', 'content': response_text})

            return APIResponse(
                message=response_text,
                output_tokens=self._count_tokens(response_text),
                prompt_tokens=self._count_tokens(message),
            )

        except Exception as e:
            raise RuntimeError(f'Error sending message: {e!s}') from e


##############################################

############# api\openai_api.py #############

from .generic_api import GenericAPI


class OpenAI_API(GenericAPI):
    """
    A specialized API wrapper for interacting with OpenAI's models.
    """

    def __init__(self, api_key: str, model: str, system_prompt: str, temperature: float = 0.85, max_tokens: int = 16383) -> None:
        """
        Initializes the OpenAI_API class with the provided configuration,
        using the default OpenAI base URL.
        """

        super().__init__(
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            base_url='https://api.openai.com/v1',
            temperature=temperature,
            max_tokens=max_tokens,
        )


#############################################

############# api\__init__.py #############



###########################################

############# data\api_response.py #############

from dataclasses import dataclass


@dataclass
class APIResponse:
    message: str
    prompt_tokens: int
    output_tokens: int


################################################

############# data\message.py #############

from dataclasses import dataclass
from typing import Literal

User = Literal['User']
Assistant = Literal['Assistant']


@dataclass
class Message:
    role: User | Assistant
    content: str


###########################################

############# utils\configs.py #############

import os
from enum import Enum
from typing import ClassVar

from dotenv import load_dotenv
from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict

# Load environment variables
load_dotenv()


# Enum definitions for models
class DeepSeekModels(Enum):
    DEEPSEEK_CHAT = 'deepseek-chat'


class QwenModels(Enum):
    QWEN_INSTRUCT = 'Qwen/Qwen2.5-72B-Instruct'


class OpenAIModels(Enum):
    CHATGPT_4O_LATEST = 'chatgpt-4o-latest'
    GPT_4O_MINI_2024_07_18 = 'gpt-4o-mini-2024-07-18'


class GeminiModels(Enum):
    GEMINI_FLASH = 'gemini-1.5-flash-002'
    GEMINI_PRO = 'gemini-1.5-pro-002'


# Settings classes with updated protected namespaces
class DeepSeekSettings(BaseSettings):
    model_config = SettingsConfigDict(protected_namespaces=('settings_',))
    model_name: DeepSeekModels = DeepSeekModels.DEEPSEEK_CHAT
    base_url: str = 'https://api.deepseek.com/beta'
    api_key: str = Field(default=os.getenv('DEEPSEEK_CHAT_API_KEY'))


class QwenSettings(BaseSettings):
    model_config = SettingsConfigDict(protected_namespaces=('settings_',))
    model_name: QwenModels = QwenModels.QWEN_INSTRUCT
    base_url: str = 'https://api.hyperbolic.xyz/v1'
    api_key: str = Field(default=os.getenv('QWEN_API_KEY'))


class OpenAISettings(BaseSettings):
    model_config = SettingsConfigDict(protected_namespaces=('settings_',))
    models: ClassVar = OpenAIModels
    api_key: str = Field(default=os.getenv('OPENAI_API_KEY'))


class GeminiSettings(BaseSettings):
    model_config = SettingsConfigDict(protected_namespaces=('settings_',))
    models: ClassVar = GeminiModels
    api_key: str = Field(default=os.getenv('GEMINI_API_KEY'))


# Create settings instances
deepseek_settings = DeepSeekSettings()
qwen_settings = QwenSettings()
openai_settings = OpenAISettings()
gemini_settings = GeminiSettings()


############################################

############# utils\functions.py #############

import os
from pathlib import Path


def read_system_prompt(name: str, directory: str = 'system_prompts') -> str | None:
    """
    Reads the content of a specific system prompt file from a specified directory.
    """

    for file in os.listdir(directory):
        file_name = Path(file).stem
        if file_name == name:
            with (Path(directory) / file).open() as f:
                return f.read().strip()
    return None


##############################################

